# Data Processing

> 참고 블로그
>
> [빅데이터 - 스칼라와 스파크로 시작](https://wikidocs.net/24693)

## 목차

- 스칼라 Scala
- 스파크 Spark



## Scala

### 정의 

- 객체 지향 언어의 특징과 함수형 언어의 특징을 함께 가지는 다중 패러다임 프로그래밍 언어



### 특징 

- JVML (Java Virtual Machine Language)

  - 자바가상머진에서 동작하는 언어 
    - Scala, Kotlin, Groovy 등등
  - 자바의 모든 라이브러리 사용 가능
  - 스칼라 컴파일러를 통해 스칼라 코드를 바이트로 변환, 바이트 코드는 JVM 위에서 자바와 동일하게 실행됨

  ![image-20211013210406240](Data Processing.assets/image-20211013210406240.png)

- 함수형 언어

  - 자바에 비해 코드 길이가 짧음
    - getter, setter, 생성자 제거하고 표현식 간소화

  - 비교

    ![image-20211013210502886](Data Processing.assets/image-20211013210502886.png)

- 바이트 코드 최적화
  - 자바보다 20% 정도 속도가 빠름
- 동시성에 강함
  - Immutable 변수가 많음
  - 순수 함수 사용하여 병렬 프로그래밍 처리에 강함



### 설치

> 추후에 추가할 예정임



## 스파크

### 정의

- 인메모리 기반의 대용량 데이터 고속 처리 엔진 범용 분산 클러스터 컴퓨팅 프레임워크



### 특징

- Speed

  - 인메모리 기반의 빠른 처리
    - 스파크는 인메모리 기반의 처리로 맵리듀스 작업처리에 비해 디스크는 10배, 메모리 작업은 100배 빠른 속도를 가지고 있습니다. 
    - 맵리듀스는 작업의 중간 결과를 디스크에 쓰기 때문에 IO로 인하여 작업 속도에 제약이 생깁니다. 
    - 스파크는 메모리에 중간 결과를 메모리에 저장하여 반복 작업의 처리 효율이 높습니다.

- Ease of Use

  - 다양한 언어 지원 (Java, Scala, Python, R, SQL)을 통한 편의성 증대

- Generality

  - SQL, Streaming, 머신러닝, 그래프 연산 등 다양한 컴포넌트 제공
  - 스파크는 자바, 스칼라, 파이썬, R 인터페이스등 다양한 언어를 지원하여 개발자에게 작업의 편의성을 제공합니다. 
  - 하지만 언어마다 처리하는 속도가 다릅니다. 따라서 성능을 위해서는 Scala 로 개발을 진행하는 것이 좋습니다.

- Run Everywhere 

  - YARN, Mesos, Kubernetes 등 다양한 클러스터에서 동작 가능
    - 클러스터 매니저로 YARN, Mesos, Kubernetes, standalone 등 다양한 포맷을 지원하여 운영 시스템 선택에 다양성을 제공합니다. 

  

  - HDFS, Casandra, HBase 등 다양한 파일 포맷 지원
    - 또한, HDFS, 카산드라, HBase, S3 등의 다양한 데이터 포맷을 지원하여 여러 시스템에 적용이 가능합니다
    - 스파크는 기본적으로 TXT, Json, ORC, Parquet 등의 파일 포맷을 지원합니다. 
    - S3, HDFS 등의 파일 시스템과 연동도 가능하고, HBase, Hive 와도 간단하게 연동할 수 있습니다.

### 컴포넌트 구성

- 스파크 라이브러리
  - Spark SQL
    - 스파크 SQL은 SQL을 이용하여 RDD, DataSet, DataFrame 작업을 생성하고 처리합니다. 
    - 하이브 메타스토어와 연결하여 하이브의 메타 정보를 이용하여 SQL 작업을 처리할 수 있습니다. 
    - 샤크(Shark)는 하이브에서 스파크 작업을 처리할 수 있도록 개발하는 외부 프로젝트 였는데 현재는 스파크 SQL로 통합되었습니다.
  - Spark Streaming
    - 스파크 스트리밍은 실시간 데이터 스트림을 처리하는 컴포넌트 입니다. 
    - 스트림 데이터를 작은 사이즈로 쪼개어 RDD 처럼 처리합니다.
  - MLib
    - MLib는 스파크 기반의 머신러닝 기능을 제공하는 컴포넌트입니다. 
    - 분류(classification), 회귀(regression), 클러스터링(clustering), 협업 필터링(collaborative filtering) 등의 머신러닝 알고리즘과 모델 평가 및 외부 데이터 불러오기 같은 기능도 지원합니다.
  - GraphX
    - GraphX는 분산형 그래프 프로세싱이 가능하게 해주는 컴포넌트입니다. 
    - 각 간선이나 점에 임의의 속성을 추가한 지향성 그래프를 만들 수 있습니다.



- 스파크 코어
  - Spark Core는 메인 컴포넌트로 작업 스케줄링, 메모리 관리, 장애 복구와 같은 기본적인 기능을 제공하고, RDD, Dateset, DataFrame을 이용한 스파크 연산을 처리합니다.
- 클러스터 매니저
  - 스파크 작업을 운영하는 클러스터 관리자 입니다. 
  - 스파크는 다양한 클러스터 매니저를 지원합니다. 
  - 스파크에서 제공하는 스탠드얼론(Standalone) 관리자를 이용할 수도 있고, 메조스(Mesos), 얀(YARN), 큐버네티스(Kubernetes) 등의 관리자를 지원합니다.

### 구조

- 드라이버 
  - 작업을 관리
- 클러스터 매니저
  - 작업이 실행되는 노드를 관리



### 스파크 어플리케이션

> 스파크 실행 프로그램으로 드라이버와 익스큐터 프로세스로 실행되는 프로그램을 말합니다. 클러스터 매니저가 스파크 애플리케이션의 리소스를 효율적으로 배분하게 됩니다.

#### 애플리케이션 구조

- 스파크 애플리케이션은 마스터-슬레이브 구조로 실행됩니다. 

![image-20211013212017061](Data Processing.assets/image-20211013212017061.png)

- 작업을 관장하는 드라이버
  - 스파크 컨텍스트 객체를 생성하여 클러스터 매니저와 통신하면서 클러스터의 자원 관리를 지원하고, 애플리케이션의 라이프 사이클을 관리합니다.
- 실제 작업이 동작하는 익스큐터



#### 드라이버

#### 익스큐터

#### 테스크



### 스파크 잡

> 스파크 애플리케이션의 작업은 잡(Job), 스테이지(Stage), 태스크(Task)로 구성됩니다.

![image-20211013212228389](Data Processing.assets/image-20211013212228389.png)



- Job
- Stage
- Task



### 클러스터 매니저

> 스파크는 여러 가지 클러스터 매니저를 지원합니다.

- Yarn
- Mesos
- StandAlone



### 모니터링

- 스파크 컨텍스트 웹 UI

- 스파크 히스토리 서버

  - 실행중인 작업과 실행이 끝난 작업의 히스토리를 확인하기 위한 서버
  - `start-history-server.sh`로 실행하고 기본 접속 포트는 `18080`

  - 영구적인 저장소에 스파크 작업 내역을 저장 하고 사용자의 요청에 정보를 반환

- REST API

  - 스파크 히스토리 서버는 작업 애플리케이션에 대해 REST API를 이용해 json 형태 정보 제공

  ```
  curl -s http://$(hostname -f):18080/api/v1/applications
  curl -s http://$(hostname -f):18080/api/v1/[app-id]/jobs
  ```

  

